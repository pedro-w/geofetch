{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geofetch tutorial\n",
    "\n",
    "The [GSE67303 data set](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE67303) has about 250 mb of data across 4 samples, so it's a quick download for a test case. Let's take a quick peek at the geofetch version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geofetch 0.7.0\n"
     ]
    }
   ],
   "source": [
    "geofetch --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see your CLI options, invoke `geofetch -h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: geofetch [-h] [-V] -i INPUT [-n NAME] [-m METADATA_ROOT]\n",
      "                [-u METADATA_FOLDER] [--just-metadata] [-r] [--acc-anno]\n",
      "                [--use-key-subset] [-x] [--config-template CONFIG_TEMPLATE]\n",
      "                [-p] [-k SKIP] [--filter FILTER] [-g GEO_FOLDER]\n",
      "                [-b BAM_FOLDER] [-f FQ_FOLDER] [-P PIPELINE_INTERFACES]\n",
      "                [--silent] [--verbosity V] [--logdev]\n",
      "\n",
      "Automatic GEO SRA data downloader\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -V, --version         show program's version number and exit\n",
      "  -i INPUT, --input INPUT\n",
      "                        required: a GEO (GSE) accession, or a file with a list\n",
      "                        of GSE numbers\n",
      "  -n NAME, --name NAME  Specify a project name. Defaults to GSE number\n",
      "  -m METADATA_ROOT, --metadata-root METADATA_ROOT\n",
      "                        Specify a parent folder location to store metadata.\n",
      "                        The project name will be added as a subfolder[Default:\n",
      "                        $SRAMETA:]\n",
      "  -u METADATA_FOLDER, --metadata-folder METADATA_FOLDER\n",
      "                        Specify an absolute folder location to store metadata.\n",
      "                        No subfolder will be added. Overrides value of\n",
      "                        --metadata-root [Default: Not used (--metadata-root is\n",
      "                        used by default)]\n",
      "  --just-metadata       If set, don't actually run downloads, just create\n",
      "                        metadata\n",
      "  -r, --refresh-metadata\n",
      "                        If set, re-download metadata even if it exists.\n",
      "  --acc-anno            Also produce annotation sheets for each accession, not\n",
      "                        just for the whole project combined\n",
      "  --use-key-subset      Use just the keys defined in this module when writing\n",
      "                        out metadata.\n",
      "  -x, --split-experiments\n",
      "                        Split SRR runs into individual samples. By default,\n",
      "                        SRX experiments with multiple SRR Runs will have a\n",
      "                        single entry in the annotation table, with each run as\n",
      "                        a separate row in the subannotation table. This\n",
      "                        setting instead treats each run as a separate sample\n",
      "  --config-template CONFIG_TEMPLATE\n",
      "                        Project config yaml file template.\n",
      "  -p, --processed       Download processed data [Default: download raw data].\n",
      "  -k SKIP, --skip SKIP  Skip some accessions. [Default: no skip].\n",
      "  --filter FILTER       Filter regex for processed filenames [Default: None].\n",
      "  -g GEO_FOLDER, --geo-folder GEO_FOLDER\n",
      "                        Optional: Specify a location to store processed GEO\n",
      "                        files [Default: $GEODATA:]\n",
      "  -b BAM_FOLDER, --bam-folder BAM_FOLDER\n",
      "                        Optional: Specify folder of bam files. Geofetch will\n",
      "                        not download sra files when corresponding bam files\n",
      "                        already exist. [Default: $SRABAM:]\n",
      "  -f FQ_FOLDER, --fq-folder FQ_FOLDER\n",
      "                        Optional: Specify folder of fastq files. Geofetch will\n",
      "                        not download sra files when corresponding fastq files\n",
      "                        already exist. [Default: $SRAFQ:]\n",
      "  -P PIPELINE_INTERFACES, --pipeline_interfaces PIPELINE_INTERFACES\n",
      "                        Optional: Specify one or more filepaths to pipeline\n",
      "                        interface yaml files. These will be added to the\n",
      "                        project config file to make it immediately compatible\n",
      "                        with looper. [Default: null]\n",
      "  --silent              Silence logging. Overrides verbosity.\n",
      "  --verbosity V         Set logging level (1-5 or logging module level name)\n",
      "  --logdev              Expand content of logging message format.\n"
     ]
    }
   ],
   "source": [
    "geofetch -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling geofetch will do 4 tasks: \n",
    "\n",
    "1. download all `.sra` files from `GSE#####` into your SRA folder (wherever you have configured `sratools` to stick data).\n",
    "2. download all metadata from GEO and SRA and store in your metadata folder.\n",
    "2. produce a PEP-compatible sample table, `PROJECT_NAME_annotation.csv`, in your metadata folder.\n",
    "3. produce a PEP-compatible project configuration file, `PROJECT_NAME_config.yaml`, in your metadata folder.\n",
    "\n",
    "Complete details about geofetch outputs is cataloged in the [metadata outputs reference](metadata_output.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "First, create the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata folder: /home/nsheff/code/geofetch/docs_jupyter/red_algae\n",
      "Trying GSE67303 (not a file) as accession...\n",
      "Skipped 0 accessions. Starting now.\n",
      "\u001b[38;5;228mProcessing accession 1 of 1: 'GSE67303'\u001b[0m\n",
      "Found previous GSE file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/GSE67303_GSE.soft\n",
      "Found previous GSM file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/GSE67303_GSM.soft\n",
      "Processed 4 samples.\n",
      "Found SRA Project accession: SRP056574\n",
      "Found previous SRA file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/GSE67303_SRA.csv\n",
      "SRP: SRP056574\n",
      "Parsing SRA file to download SRR records\n",
      "Get SRR: SRR1930183 (SRX969073)\n",
      "Dry run (no data download)\n",
      "Get SRR: SRR1930184 (SRX969074)\n",
      "Dry run (no data download)\n",
      "Get SRR: SRR1930185 (SRX969075)\n",
      "Dry run (no data download)\n",
      "Get SRR: SRR1930186 (SRX969076)\n",
      "Dry run (no data download)\n",
      "Finished processing 1 accession(s)\n",
      "Creating complete project annotation sheets and config file...\n",
      "Sample annotation sheet: /home/nsheff/code/geofetch/docs_jupyter/red_algae/red_algae_annotation.csv\n",
      "Writing: /home/nsheff/code/geofetch/docs_jupyter/red_algae/red_algae_annotation.csv\n",
      "  Config file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/red_algae_config.yaml\n"
     ]
    }
   ],
   "source": [
    "geofetch -i GSE67303 -n red_algae -m `pwd` --just-metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-m` parameter specifies to use the current directory, storing the data according to the name (`-n`) parameter. So, we'll now have a `red_alga` subfolder, where the results will be saved. Inside that folder you'll see the output of the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSE67303_GSE.soft  GSE67303_SRA.csv       red_algae_annotation.csv  \u001b[0m\u001b[01;34msubmission\u001b[0m\n",
      "GSE67303_GSM.soft  GSE67303_SRA_filt.csv  red_algae_config.yaml\n"
     ]
    }
   ],
   "source": [
    "ls red_algae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.soft` files are the direct output from GEO, which contain all the metadata as stored by GEO, for both the experiment (`_GSE`) and for the individual samples (`_GSM`). Geofetch also produces a `csv` file with the SRA metadata. The filtered version (ending in `_filt`) would contain only the specified subset of the samples if we didn't request them all, but in this case, since we only gave an accession, it is identical to the complete file.\n",
    "\n",
    "Finally, there are the 2 files that make up the PEP: the `_config.yaml` file and the `_annotation.csv` file. Let's see what's in these files now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by geofetch\n",
      "\n",
      "name: red_algae\n",
      "pep_version: 2.0.0\n",
      "sample_table: red_algae_annotation.csv\n",
      "subsample_table: null\n",
      "\n",
      "looper:\n",
      "  output_dir: red_algae\n",
      "  pipeline_interfaces: null\n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    SRR_files: SRA\n",
      "    pipeline_interfaces: null\n",
      "  derive:\n",
      "    attributes: [read1, read2, SRR_files]\n",
      "    sources:\n",
      "      SRA: \"${SRABAM}/{SRR}.bam\"\n",
      "      FQ: \"${SRAFQ}/{SRR}.fastq.gz\"\n",
      "      FQ1: \"${SRAFQ}/{SRR}_1.fastq.gz\"\n",
      "      FQ2: \"${SRAFQ}/{SRR}_2.fastq.gz\"      \n",
      "  imply:\n",
      "    - if: \n",
      "        organism: \"Mus musculus\"\n",
      "      then:\n",
      "        genome: mm10\n",
      "    - if: \n",
      "        organism: \"Homo sapiens\"\n",
      "      then:\n",
      "        genome: hg38          \n",
      "    - if: \n",
      "        read_type: \"PAIRED\"\n",
      "      then:\n",
      "        read1: FQ1\n",
      "        read2: FQ2          \n",
      "    - if: \n",
      "        read_type: \"SINGLE\"\n",
      "      then:\n",
      "        read1: FQ1\n",
      "\n",
      "project_modifiers:\n",
      "  amend:\n",
      "    sra_convert:\n",
      "      looper:\n",
      "        results_subdir: sra_convert_results      \n",
      "      sample_modifiers:\n",
      "        append:\n",
      "          SRR_files: SRA\n",
      "          pipeline_interfaces: ${CODE}/geofetch/pipeline_interface_convert.yaml\n",
      "        derive:\n",
      "          attributes: [read1, read2, SRR_files]\n",
      "          sources:\n",
      "            SRA: \"${SRARAW}/{SRR}.sra\"\n",
      "            FQ: \"${SRAFQ}/{SRR}.fastq.gz\"\n",
      "            FQ1: \"${SRAFQ}/{SRR}_1.fastq.gz\"\n",
      "            FQ2: \"${SRAFQ}/{SRR}_2.fastq.gz\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat red_algae/red_algae_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important things to note in his file: First, see in the PEP that `sample_table` points to the csv file produced by geofetch. Second, look at the amendment called `sra_convert`. This adds a pipeline interface to the sra conversion pipeline, and adds derived attributes for SRA files and fastq files that rely on environment variables called `$SRARAW` and `$SRAFQ`. These environment variables should point to folders where you store your raw .sra files and the converted fastq files.\n",
    "\n",
    "Now let's look at the first 100 characters of the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_name,protocol,organism,read_type,data_source,SRR,SRX,Sample_title,Sample_geo_accession,Sample\n",
      "Cm_BlueLight_Rep1,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930183,SRX969073,Cm_BlueLig\n",
      "Cm_BlueLight_Rep2,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930184,SRX969074,Cm_BlueLig\n",
      "Cm_Darkness_Rep1,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930185,SRX969075,Cm_Darkness\n",
      "Cm_Darkness_Rep2,cDNA,Cyanidioschyzon merolae strain 10D,PAIRED,SRA,SRR1930186,SRX969076,Cm_Darkness\n"
     ]
    }
   ],
   "source": [
    "cut -c -100 red_algae/red_algae_annotation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata folder: /home/nsheff/code/geofetch/docs_jupyter/red_algae\n",
      "Trying GSE67303 (not a file) as accession...\n",
      "Skipped 0 accessions. Starting now.\n",
      "\u001b[38;5;228mProcessing accession 1 of 1: 'GSE67303'\u001b[0m\n",
      "Found previous GSE file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/GSE67303_GSE.soft\n",
      "Found previous GSM file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/GSE67303_GSM.soft\n",
      "Processed 4 samples.\n",
      "Found SRA Project accession: SRP056574\n",
      "Found previous SRA file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/GSE67303_SRA.csv\n",
      "SRP: SRP056574\n",
      "Parsing SRA file to download SRR records\n",
      "Get SRR: SRR1930183 (SRX969073)\n",
      "\n",
      "2020-05-21T20:20:24 prefetch.2.10.0: 1) Downloading 'SRR1930183'...\n",
      "2020-05-21T20:20:24 prefetch.2.10.0:  Downloading via https...\n",
      "2020-05-21T20:24:56 prefetch.2.10.0:  https download succeed\n",
      "2020-05-21T20:24:56 prefetch.2.10.0: 1) 'SRR1930183' was downloaded successfully\n",
      "2020-05-21T20:24:56 prefetch.2.10.0: 'SRR1930183' has 0 unresolved dependencies\n",
      "Get SRR: SRR1930184 (SRX969074)\n",
      "\n",
      "2020-05-21T20:24:58 prefetch.2.10.0: 1) Downloading 'SRR1930184'...\n",
      "2020-05-21T20:24:58 prefetch.2.10.0:  Downloading via https...\n",
      "2020-05-21T20:27:20 prefetch.2.10.0:  https download succeed\n",
      "2020-05-21T20:27:20 prefetch.2.10.0: 1) 'SRR1930184' was downloaded successfully\n",
      "2020-05-21T20:27:20 prefetch.2.10.0: 'SRR1930184' has 0 unresolved dependencies\n",
      "Get SRR: SRR1930185 (SRX969075)\n",
      "\n",
      "2020-05-21T20:27:21 prefetch.2.10.0: 1) Downloading 'SRR1930185'...\n",
      "2020-05-21T20:27:21 prefetch.2.10.0:  Downloading via https...\n",
      "2020-05-21T20:33:40 prefetch.2.10.0:  https download succeed\n",
      "2020-05-21T20:33:40 prefetch.2.10.0: 1) 'SRR1930185' was downloaded successfully\n",
      "2020-05-21T20:33:40 prefetch.2.10.0: 'SRR1930185' has 0 unresolved dependencies\n",
      "Get SRR: SRR1930186 (SRX969076)\n",
      "\n",
      "2020-05-21T20:33:42 prefetch.2.10.0: 1) Downloading 'SRR1930186'...\n",
      "2020-05-21T20:33:42 prefetch.2.10.0:  Downloading via https...\n",
      "2020-05-21T20:38:41 prefetch.2.10.0:  https download succeed\n",
      "2020-05-21T20:38:41 prefetch.2.10.0: 1) 'SRR1930186' was downloaded successfully\n",
      "2020-05-21T20:38:41 prefetch.2.10.0: 'SRR1930186' has 0 unresolved dependencies\n",
      "Finished processing 1 accession(s)\n",
      "Creating complete project annotation sheets and config file...\n",
      "Sample annotation sheet: /home/nsheff/code/geofetch/docs_jupyter/red_algae/red_algae_annotation.csv\n",
      "Writing: /home/nsheff/code/geofetch/docs_jupyter/red_algae/red_algae_annotation.csv\n",
      "  Config file: /home/nsheff/code/geofetch/docs_jupyter/red_algae/red_algae_config.yaml\n"
     ]
    }
   ],
   "source": [
    "geofetch -i GSE67303 -n red_algae -m `pwd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to fastq format\n",
    "\n",
    "Now the `.sra` files have been downloaded. The project that was automatically created by GEO contained an amendment for sra file conversion. This project expects you to have an environment variable called `SRARAW` that points to the location where `prefetch` stores your `.sra` files. We also should define a `$SRAFQ` variable to point to where we ant the fastq files stored. In this command below, we set these on the fly for this command, but you can also just use globals.\n",
    "\n",
    "We'll use `-d` first to do a dry run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.2.0-dev\n",
      "Command: run\n",
      "Using amendments: sra_convert\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 4] sample: Cm_BlueLight_Rep1; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_BlueLight_Rep1.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_BlueLight_Rep1.sub\n",
      "Dry run, not submitted\n",
      "\u001b[36m## [2 of 4] sample: Cm_BlueLight_Rep2; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_BlueLight_Rep2.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_BlueLight_Rep2.sub\n",
      "Dry run, not submitted\n",
      "\u001b[36m## [3 of 4] sample: Cm_Darkness_Rep1; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_Darkness_Rep1.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_Darkness_Rep1.sub\n",
      "Dry run, not submitted\n",
      "\u001b[36m## [4 of 4] sample: Cm_Darkness_Rep2; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_Darkness_Rep2.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_Darkness_Rep2.sub\n",
      "Dry run, not submitted\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 4 of 4\n",
      "Commands submitted: 4 of 4\n",
      "Jobs submitted: 4\n",
      "Dry run. No jobs were actually submitted.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "SRARAW=${HOME}/ncbi/public/sra/ SRAFQ=red_algae/fastq \\\n",
    "  looper run red_algae/red_algae_config.yaml -a sra_convert -p local -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the real thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.2.0-dev\n",
      "Command: run\n",
      "Using amendments: sra_convert\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 4] sample: Cm_BlueLight_Rep1; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_BlueLight_Rep1.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_BlueLight_Rep1.sub\n",
      "Compute node: zither\n",
      "Start time: 2020-05-21 17:40:56\n",
      "Using outfolder: red_algae/results_pipeline/SRX969073\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/nsheff/.local/bin/sraconvert --srr /home/nsheff/ncbi/public/sra//SRR1930183.sra --sample-name SRX969073 -O red_algae/results_pipeline --keep-sra`\n",
      "*         Compute host:  zither\n",
      "*          Working dir:  /home/nsheff/code/geofetch/docs_jupyter\n",
      "*            Outfolder:  red_algae/results_pipeline/SRX969073/\n",
      "*  Pipeline started at:   (05-21 17:40:57) elapsed: 0.0 _TIME_\n",
      "\n",
      "### Version log:\n",
      "\n",
      "*       Python version:  3.7.5\n",
      "*          Pypiper dir:  `/home/nsheff/.local/lib/python3.7/site-packages/pypiper`\n",
      "*      Pypiper version:  0.12.1\n",
      "*         Pipeline dir:  `/home/nsheff/.local/bin`\n",
      "*     Pipeline version:  None\n",
      "\n",
      "### Arguments passed to pipeline:\n",
      "\n",
      "*          `bamfolder`:  ``\n",
      "*        `config_file`:  `sraconvert.yaml`\n",
      "*             `format`:  `fastq`\n",
      "*           `fqfolder`:  `red_algae/fastq`\n",
      "*           `keep_sra`:  `True`\n",
      "*             `logdev`:  `False`\n",
      "*               `mode`:  `convert`\n",
      "*      `output_parent`:  `red_algae/results_pipeline`\n",
      "*            `recover`:  `False`\n",
      "*        `sample_name`:  `['SRX969073']`\n",
      "*             `silent`:  `False`\n",
      "*          `srafolder`:  `/home/nsheff/ncbi/public/sra/`\n",
      "*                `srr`:  `['/home/nsheff/ncbi/public/sra//SRR1930183.sra']`\n",
      "*          `verbosity`:  `None`\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Processing 1 of 1 files: SRR1930183\n",
      "Target to produce: `red_algae/fastq/SRR1930183_1.fastq.gz`  \n",
      "\n",
      "> `fastq-dump /home/nsheff/ncbi/public/sra//SRR1930183.sra --split-files --gzip -O red_algae/fastq` (9436)\n",
      "<pre>\n",
      "Read 1068319 spots for /home/nsheff/ncbi/public/sra//SRR1930183.sra\n",
      "Written 1068319 spots for /home/nsheff/ncbi/public/sra//SRR1930183.sra\n",
      "</pre>\n",
      "Command completed. Elapsed time: 0:00:38. Running peak memory: 0.067GB.  \n",
      "  PID: 9436;\tCommand: fastq-dump;\tReturn code: 0;\tMemory used: 0.067GB\n",
      "\n",
      "Already completed files: []\n",
      "\n",
      "### Pipeline completed. Epilogue\n",
      "*        Elapsed time (this run):  0:00:38\n",
      "*  Total elapsed time (all runs):  0:00:38\n",
      "*         Peak memory (this run):  0.0666 GB\n",
      "*        Pipeline completed time: 2020-05-21 17:41:35\n",
      "\u001b[36m## [2 of 4] sample: Cm_BlueLight_Rep2; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_BlueLight_Rep2.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_BlueLight_Rep2.sub\n",
      "Compute node: zither\n",
      "Start time: 2020-05-21 17:41:36\n",
      "Using outfolder: red_algae/results_pipeline/SRX969074\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/nsheff/.local/bin/sraconvert --srr /home/nsheff/ncbi/public/sra//SRR1930184.sra --sample-name SRX969074 -O red_algae/results_pipeline --keep-sra`\n",
      "*         Compute host:  zither\n",
      "*          Working dir:  /home/nsheff/code/geofetch/docs_jupyter\n",
      "*            Outfolder:  red_algae/results_pipeline/SRX969074/\n",
      "*  Pipeline started at:   (05-21 17:41:36) elapsed: 0.0 _TIME_\n",
      "\n",
      "### Version log:\n",
      "\n",
      "*       Python version:  3.7.5\n",
      "*          Pypiper dir:  `/home/nsheff/.local/lib/python3.7/site-packages/pypiper`\n",
      "*      Pypiper version:  0.12.1\n",
      "*         Pipeline dir:  `/home/nsheff/.local/bin`\n",
      "*     Pipeline version:  None\n",
      "\n",
      "### Arguments passed to pipeline:\n",
      "\n",
      "*          `bamfolder`:  ``\n",
      "*        `config_file`:  `sraconvert.yaml`\n",
      "*             `format`:  `fastq`\n",
      "*           `fqfolder`:  `red_algae/fastq`\n",
      "*           `keep_sra`:  `True`\n",
      "*             `logdev`:  `False`\n",
      "*               `mode`:  `convert`\n",
      "*      `output_parent`:  `red_algae/results_pipeline`\n",
      "*            `recover`:  `False`\n",
      "*        `sample_name`:  `['SRX969074']`\n",
      "*             `silent`:  `False`\n",
      "*          `srafolder`:  `/home/nsheff/ncbi/public/sra/`\n",
      "*                `srr`:  `['/home/nsheff/ncbi/public/sra//SRR1930184.sra']`\n",
      "*          `verbosity`:  `None`\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Processing 1 of 1 files: SRR1930184\n",
      "Target exists: `red_algae/fastq/SRR1930184_1.fastq.gz`  \n",
      "Already completed files: []\n",
      "\n",
      "### Pipeline completed. Epilogue\n",
      "*        Elapsed time (this run):  0:00:00\n",
      "*  Total elapsed time (all runs):  0:00:00\n",
      "*         Peak memory (this run):  0 GB\n",
      "*        Pipeline completed time: 2020-05-21 17:41:36\n",
      "\u001b[36m## [3 of 4] sample: Cm_Darkness_Rep1; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_Darkness_Rep1.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_Darkness_Rep1.sub\n",
      "Compute node: zither\n",
      "Start time: 2020-05-21 17:41:36\n",
      "Using outfolder: red_algae/results_pipeline/SRX969075\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/nsheff/.local/bin/sraconvert --srr /home/nsheff/ncbi/public/sra//SRR1930185.sra --sample-name SRX969075 -O red_algae/results_pipeline --keep-sra`\n",
      "*         Compute host:  zither\n",
      "*          Working dir:  /home/nsheff/code/geofetch/docs_jupyter\n",
      "*            Outfolder:  red_algae/results_pipeline/SRX969075/\n",
      "*  Pipeline started at:   (05-21 17:41:36) elapsed: 0.0 _TIME_\n",
      "\n",
      "### Version log:\n",
      "\n",
      "*       Python version:  3.7.5\n",
      "*          Pypiper dir:  `/home/nsheff/.local/lib/python3.7/site-packages/pypiper`\n",
      "*      Pypiper version:  0.12.1\n",
      "*         Pipeline dir:  `/home/nsheff/.local/bin`\n",
      "*     Pipeline version:  None\n",
      "\n",
      "### Arguments passed to pipeline:\n",
      "\n",
      "*          `bamfolder`:  ``\n",
      "*        `config_file`:  `sraconvert.yaml`\n",
      "*             `format`:  `fastq`\n",
      "*           `fqfolder`:  `red_algae/fastq`\n",
      "*           `keep_sra`:  `True`\n",
      "*             `logdev`:  `False`\n",
      "*               `mode`:  `convert`\n",
      "*      `output_parent`:  `red_algae/results_pipeline`\n",
      "*            `recover`:  `False`\n",
      "*        `sample_name`:  `['SRX969075']`\n",
      "*             `silent`:  `False`\n",
      "*          `srafolder`:  `/home/nsheff/ncbi/public/sra/`\n",
      "*                `srr`:  `['/home/nsheff/ncbi/public/sra//SRR1930185.sra']`\n",
      "*          `verbosity`:  `None`\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Processing 1 of 1 files: SRR1930185\n",
      "Target to produce: `red_algae/fastq/SRR1930185_1.fastq.gz`  \n",
      "\n",
      "> `fastq-dump /home/nsheff/ncbi/public/sra//SRR1930185.sra --split-files --gzip -O red_algae/fastq` (9607)\n",
      "<pre>\n",
      "Read 1707508 spots for /home/nsheff/ncbi/public/sra//SRR1930185.sra\n",
      "Written 1707508 spots for /home/nsheff/ncbi/public/sra//SRR1930185.sra\n",
      "</pre>\n",
      "Command completed. Elapsed time: 0:01:01. Running peak memory: 0.066GB.  \n",
      "  PID: 9607;\tCommand: fastq-dump;\tReturn code: 0;\tMemory used: 0.066GB\n",
      "\n",
      "Already completed files: []\n",
      "\n",
      "### Pipeline completed. Epilogue\n",
      "*        Elapsed time (this run):  0:01:01\n",
      "*  Total elapsed time (all runs):  0:01:01\n",
      "*         Peak memory (this run):  0.0656 GB\n",
      "*        Pipeline completed time: 2020-05-21 17:42:37\n",
      "\u001b[36m## [4 of 4] sample: Cm_Darkness_Rep2; pipeline: sra_convert\u001b[0m\n",
      "Writing script to /home/nsheff/code/geofetch/docs_jupyter/red_algae/submission/sra_convert_Cm_Darkness_Rep2.sub\n",
      "Job script (n=1; 0.00Gb): red_algae/submission/sra_convert_Cm_Darkness_Rep2.sub\n",
      "Compute node: zither\n",
      "Start time: 2020-05-21 17:42:38\n",
      "Using outfolder: red_algae/results_pipeline/SRX969076\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/nsheff/.local/bin/sraconvert --srr /home/nsheff/ncbi/public/sra//SRR1930186.sra --sample-name SRX969076 -O red_algae/results_pipeline --keep-sra`\n",
      "*         Compute host:  zither\n",
      "*          Working dir:  /home/nsheff/code/geofetch/docs_jupyter\n",
      "*            Outfolder:  red_algae/results_pipeline/SRX969076/\n",
      "*  Pipeline started at:   (05-21 17:42:38) elapsed: 0.0 _TIME_\n",
      "\n",
      "### Version log:\n",
      "\n",
      "*       Python version:  3.7.5\n",
      "*          Pypiper dir:  `/home/nsheff/.local/lib/python3.7/site-packages/pypiper`\n",
      "*      Pypiper version:  0.12.1\n",
      "*         Pipeline dir:  `/home/nsheff/.local/bin`\n",
      "*     Pipeline version:  None\n",
      "\n",
      "### Arguments passed to pipeline:\n",
      "\n",
      "*          `bamfolder`:  ``\n",
      "*        `config_file`:  `sraconvert.yaml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*             `format`:  `fastq`\n",
      "*           `fqfolder`:  `red_algae/fastq`\n",
      "*           `keep_sra`:  `True`\n",
      "*             `logdev`:  `False`\n",
      "*               `mode`:  `convert`\n",
      "*      `output_parent`:  `red_algae/results_pipeline`\n",
      "*            `recover`:  `False`\n",
      "*        `sample_name`:  `['SRX969076']`\n",
      "*             `silent`:  `False`\n",
      "*          `srafolder`:  `/home/nsheff/ncbi/public/sra/`\n",
      "*                `srr`:  `['/home/nsheff/ncbi/public/sra//SRR1930186.sra']`\n",
      "*          `verbosity`:  `None`\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Processing 1 of 1 files: SRR1930186\n",
      "Target to produce: `red_algae/fastq/SRR1930186_1.fastq.gz`  \n",
      "\n",
      "> `fastq-dump /home/nsheff/ncbi/public/sra//SRR1930186.sra --split-files --gzip -O red_algae/fastq` (9780)\n",
      "<pre>\n",
      "Read 1224029 spots for /home/nsheff/ncbi/public/sra//SRR1930186.sra\n",
      "Written 1224029 spots for /home/nsheff/ncbi/public/sra//SRR1930186.sra\n",
      "</pre>\n",
      "Command completed. Elapsed time: 0:00:44. Running peak memory: 0.067GB.  \n",
      "  PID: 9780;\tCommand: fastq-dump;\tReturn code: 0;\tMemory used: 0.067GB\n",
      "\n",
      "Already completed files: []\n",
      "\n",
      "### Pipeline completed. Epilogue\n",
      "*        Elapsed time (this run):  0:00:44\n",
      "*  Total elapsed time (all runs):  0:00:44\n",
      "*         Peak memory (this run):  0.0673 GB\n",
      "*        Pipeline completed time: 2020-05-21 17:43:22\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 4 of 4\n",
      "Commands submitted: 4 of 4\n",
      "Jobs submitted: 4\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "SRARAW=${HOME}/ncbi/public/sra/ SRAFQ=red_algae/fastq \\\n",
    "  looper run red_algae/red_algae_config.yaml -a sra_convert -p local \\\n",
    "  --command-extra=--keep-sra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's done, let's take a look in the `red_algae/fastq` folder (where we set the `$SRAFQ` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mSRR1930183_1.fastq.gz\u001b[0m  \u001b[01;31mSRR1930184_2.fastq.gz\u001b[0m  \u001b[01;31mSRR1930186_1.fastq.gz\u001b[0m\n",
      "\u001b[01;31mSRR1930183_2.fastq.gz\u001b[0m  \u001b[01;31mSRR1930185_1.fastq.gz\u001b[0m  \u001b[01;31mSRR1930186_2.fastq.gz\u001b[0m\n",
      "\u001b[01;31mSRR1930184_1.fastq.gz\u001b[0m  \u001b[01;31mSRR1930185_2.fastq.gz\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls red_algae/fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the sra conversion script will delete the `.sra` files after they have been converted to fastq. You can keep them if you want by passing `--keep-sra`, which you can do by passing `--command-extra=--keep-sra` to your `looper run` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finalize the project config and sample annotation\n",
    "\n",
    "That's basically it! `geofetch` will have produced a general-purpose PEP for you, but you'll need to modify it for whatever purpose you have. For example, one common thing is to link to the pipeline you want to use by adding a `pipeline_interface` to the project config file. You may also need to adjust the `sample_annotation` file to make sure you have the right column names and attributes needed by the pipeline you're using. GEO submitters are notoriously bad at getting the metadata correct.\n",
    "\n",
    "\n",
    "## Selecting samples to download.\n",
    "\n",
    "By default, `geofetch` downloads all the data for one accession of interest. If you need more fine-grained control, either because you have multiple accessions or you need a subset of samples within them, you can use the [file-based sample specification](file-specification.md).\n",
    "\n",
    "\n",
    "## Tips\n",
    "\n",
    "* Set an environment variable for `$SRABAM` (where `.bam` files will live), and `geofetch` will check to see if you have an already-converted bamfile there before issuing the command to download the `sra` file. In this way, you can delete old `sra` files after conversion and not have to worry about re-downloading them. \n",
    "\n",
    "* The config template uses an environment variable `$SRARAW` for where `.sra` files will live. If you set this variable to the same place you instructed `sratoolkit` to download `sra` files, you won't have to tweak the config file. For more information refer to the [`sratools` page](howto-location.md).\n",
    "\n",
    "You can find a complete example of [using `geofetch` for RNA-seq data](https://github.com/databio/example-projects/tree/master/rna-seq). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
